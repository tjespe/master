\section{Literature}
\label{sec:2_literature}

\subsection{Traditional Approaches to Tail Risk}
\label{sec:2.1_traditional}

The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model introduced by \textcite{bollerslev1986garch} remains a foundational framework for modeling time-varying volatility and forecasting tail risk. Extensions such as the GJR-GARCH by \textcite{GLOSTEN1993}, designed to capture leverage effects, and the EGARCH model by \textcite{Nelson1991egarch}, aimed at asymmetric volatility shocks, have been widely utilized in the financial literature and remains effective contenders for tail risk forecasting. Early implementations of these models commonly assumed normally distributed returns, which has been shown to be unrealistic given the empirical evidence of heavy tails and possible skewness in financial returns \parencite{Peir1994}. To better address this, GARCH variants implementing heavy-tailed or skewed distributions, such as the Student-t and Skewed-t, have been extensively adopted and empirically often demonstrate improved adequacy and accuracy in tail risk forecasting \parencite{Giot2004,Angelidis2004,Slim2017, Bayer2020, Braione2016}. Although \textcite{Slim2017} point out that the normality assumption works well in calm times, it is not adequate in more volatile and extreme conditions, which are captured better by heavy-tailed distributions. Nonetheless, any fixed parametric assumption offers limited flexibility, as changing market conditions can make the true underlying distributions of financial returns can vary dynamically and substantially over time. 

In response to the industry-wide shift from VaR to ES as the preferred measure for regulatory and risk management purposes, researchers have recently developed econometric models specifically targeting joint VaR and ES forecasting. Traditionally, a key challenge has been the fact that ES alone is not elicitable, meaning there exists no strictly consistent scoring function \parencite{Gneiting2011}, complicating its estimation and validation. However, \textcite{fissler2016} proved the joint elicitability of VaR and ES, leading to new composite loss functions-such as the FZ-loss they propose—for simultaneous estimation. Building on this, \textcite{Taylor2017} construct an Asymmetric Laplace (AL) scoring method for joint evaluation. These advancements have sparked interest in semi-parametric quantile regression approaches for jointly modeling VaR and ES, which have shown strong empirical performance relative to traditional methods.

Following the introduction of the AL-score, \textcite{Taylor2017} develop a quantile regression model for VaR, linking ES to the time-varying scale parameter of the asymmetric Laplace density. This approach demonstrates competitive performance relative to GARCH extensions in estimating ES. Similarly, \textcite{chao2018} propose a Realized-ES-CAViaR framework, integrating high-frequency realized volatility measures within quantile regression and measurement equations, leading to enhanced predictive performance of ES evaluated by FZ- and AL-scores. Expanding on joint regression methodologies, \textcite{Patton2019} use a Generalized Autoregressive Score (GAS) model optimized using the FZ-loss function, achieving superior predictive accuracy compared to conventional GARCH benchmarks. Perhaps most notably, \textcite{Dimitriadis_2019db} introduce a regression-based model utilizing robust M-estimation procedures alongside the FZ-loss function, consistently outperforming benchmarks such as Historical Simulation (HS), GARCH, and HAR models.

A key advantage of these semi-parametric quantile-based frameworks over GARCH-based models is their aviodance of distributional assumptions, allowing the dynamics of the quantiles to vary across probability levels \parencite{Taylor2017}. However, these models are still constrained by the predefined functional forms and parametric specifications used to describe the dynamics and relationship of VaR and ES. Consequently, the traditional approaches to tail risk estimation, while interpretable and well-established, can struggle to model highly non-linear relationships or to adapt flexibly to complex, time-varying distributional structures.  

\subsection{Uncertainty Quantification in Financial Time Series using Probabilistic AI}
\label{sec:2.2_probai}

The use of probabilistic AI for uncertainty quantification in financial time series forecasting has witnessed a significant surge in scientific output over the past few years \parencite{Blasco2024}, with the number of published articles more than doubling since 2020 \parencite{eggen2025probabilistic}. The probabilistic modeling approach uses probability theory to express uncertainty in predictions or estimate full probability distributions over possible outcomes rather then single point predictions \parencite{Ghahramani2015}. Machine learning models in such a framework have several interesting properties addressing the limitations of the traditional approaches mentioned in \ref{sec:2.1_traditional}, including the ability to handle complex, non-linear data structures and no reliance on parametric assumptions. Additionally, by allowing for flexible, non-parametric distributional outputs, direct estimation of tail risk measures such as Value-at-Risk (VaR) and Expected Shortfall (ES) is possible, without assuming any predefined relationship between them.

When quantifying uncertainty in financial time series forecasting, it is essential to distinguish between epistemic and aleatoric uncertainty. Epistemic uncertainty arises from limitations in the model itself—such as misspecification, insufficient data, or missing features—and is reducible through better modeling and data collection \parencite{Hullermeier2021}. In contrast, aleatoric uncertainty reflects inherent randomness in the data. In finance, this corresponds to latent volatility caused by unpredictable market behavior, economic shocks, or investor sentiment, and is fundamentally irreducible \parencite{murphy_probai_2022}. This distinction is particularly relevant in financial applications, where epistemic uncertainty signals the model’s confidence in its own predictions, while aleatoric uncertainty reflects actual financial risk. However, our preliminary systematic review of probabilistic models for uncertainty quantification in financial time series forecasting find that only a small minority of studies attempt to disentangle these two types of uncertainty in a meaningful way \parencite{eggen2025probabilistic}. This observation is consistent with the findings of \textcite{Blasco2024}, who note that most authors in the field report predictive uncertainty without interpreting its underlying source. 

There are several ways in which a machine learning model can be made probabilistic by quantifying predictive uncertainty. In neural networks, aleatoric uncertainty is typically captured by modifying the output layer to parameterize a probability distribution over the target variable using a suitable likelihood-based loss function \parencite{jospin2022hands}, such as the negative log-likelihood (NLL) proposed by \textcite{lakshminarayanan2017simple}. For instance, \textcite{Horta2024} employ a Bayesian Neural Network (BNN) with this structure to forecast the VIX with an aleatoric uncertainty estimate. To additionally quantify epistemic uncertainty, Bayesian frameworks introduce uncertainty over model parameters by placing prior distributions on the weights and performing posterior inference. This can be done through techniques such as variational inference or Bayes by Backprop \parencite{blundell2015weight, kendall2017uncertainties}, or approximated via Monte Carlo Dropout during inference, as proposed by \textcite{gal2016dropout}. Within a Bayesian framework, \textcite{depeweg2018uncertainty_decomposition} derive a formal decomposition of predictive uncertainty using the law of total variance: \begin{equation}
    %\begin{footnotesize}
    \begin{gathered} \underbrace{V(y^*|x^*,\mathcal{D})}_{\displaystyle\text{\small Total uncertainty}} =
    \underbrace{\mathbb{E}_{\mathbf{w}\sim p(\mathbf{w}|\mathcal{D})}\big[V(y^*| x^*,\mathbf{w})\big]}_{\displaystyle\text{\small Aleatoric uncertainty}}+
    \underbrace{V_{\mathbf{w}\sim p(\mathbf{w}|\mathcal{D})}\big[\mathbb{E}(y^*| x^*,\mathbf{w})\big]}_{\displaystyle\text{\small Epistemic uncertainty}}
    \end{gathered}
    %\end{footnotesize}
\end{equation}
Here $y^*$ is the output prediction for input features $x^*$, given a dataset $\mathcal{D}=\left\{(x_i, y_i)\right\}_{i=1}^{N}$ with feature vectors $x_i$ and targets $y_i$, and $\mathbf{w}$ are the model weights treated as random variables distributed according to the posterior distribution $p(\mathbf{w} \mid \mathcal{D}).$ However, \textcite{lakshminarayanan2017simple} note that Bayesian techniques require significant modifications to the training procedure and are computationally expensive relative to standard, non-Bayesian neural networks. As a scalable alternative, ensembling techniques-where multiple models are trained with different random initializations or bootstrapped data splits-have been shown to approximate epistemic uncertainty effectively through the variance across their predictions \parencite{liu2019accurate, wang2025aleatoric, Berry2023decomposeALandEP}. 

An alternative method of making a neural network probabilistic is by reformulating its outputs to predictive distributions instead of point estimates using a mixture density model in the output layer, such as the Mixture Density Network (MDN) proposed by \textcite{bishop1994mdn}. A MDN is a neural network architecture outputting the parameters of a mixture distribution—typically Gaussian—and is especially useful for generating non-parametric distributions as the model with enough mixture components in principle can represent arbitrary continuous conditional probability distributions \parencite{bishop1994mdn}. Theoretically, this makes the architecture well-suited for capturing the heavy tails, skewness and time-varying volatility dynamics of financial data. 

Despite this conceptual promise, MDNs have seen limited application in the financial time series research, especially in the context of tail risk. Early adaptations of MDNs to financial contexts leveraged recurrent neural networks to incorporate time-series memory, motivated by the ability to learn time-varying distributions and allowing the model to dynamically adapt to evolving patterns \parencite{Nikolaev2013}. \textcite{Schittenkopf2000} were the first to propose Recurrent Mixture Density Networks (RMDNs) specifically for volatility forecasting, with \textcite{Nikolaev2013} later demonstrating improved predictive performance using RMDN, achieving superior log-likelihood scores and more accurate conditional variance forecasts compared to GARCH models. More recently, the integration of MDNs with advanced recurrent architectures such as Long Short-Term Memory (LSTM) networks has begun attracting attention for VaR estimation. \textcite{arimond2020} deployed an LSTM-MDN with a two-component Gaussian mixture designed to capture distinct market regimes ("bull" versus "bear") and reported fewer VaR exceedances in equity and bond indices relative to traditional methods. However, adequacy tests on the estimates are not conducted. Building on this approach, \textcite{karlsson2021value} also applied a two component LSTM-MDN to VaR estimation across several stocks, explicitly assessing adequacy via Christoffersen’s conditional coverage test. They find that the LSTM-MDN consistently overestimated losses and failed adequacy testing, though they acknowledge the model’s superior responsiveness to volatility shifts compared to traditional benchmarks, warranting further research.  Most recently, \textcite{herrig} conducted a comprehensive evaluation of a three-component LSTM-MDN for VaR forecasting on stock indices. Findings indicated that LSTM-MDNs underperformed during stable periods but provided notably improved forecasts during the highly volatile market conditions experienced in 2021–22, even surpassing GARCH and historical simulation benchmarks. Additionally, LSTM-MDN approaches tend to generate overly conservative risk estimates, but careful modeling choices could enhance their practical utility and warrents further research. To date however, there are no published studies to our knowledge employing MDN-based approaches for Expected Shortfall (ES) estimation, leaving a substantial gap in the exploration of these models for comprehensive tail-risk modeling.

%In neural networks, this is often implemented by having the model predict both the conditional mean and variance of the output and training it with a suitable likelihood-based loss function, such as the negative log-likelihood under a Gaussian assumption




%There are several ways in which a model can be made probabilistic by quantifying these types of uncertainties ... commonly including techniques for quantifying epistemic uncertainty such as variational inference and ensembling, or techniques for quantifying aleatoric uncertainty, such as modifying the models to output random variables rather than point predictions, e.g. through modifying a neural network to output both a mean and a variance and selecting an appropriate loss function [relevant reference].

In our preliminary systematic review \parencite{eggen2025probabilistic}, we find that the most frequently applied probabilistic AI models used for uncertainty quantification in financial time series are Gaussian Process Regression (GPR), Bayesian Neural Networks (BNNs), and probabilistic extensions of Recurrent Neural Networks (P-RNNs). While the number of publications in this area has increased substantially in recent years, the literature remains fragmented, with limited methodological consistency and little accumulation of results across studies. First, comprehensive benchmarking against traditional econometric models is rare, making it difficult to assess whether probabilistic models actually improve upon established approaches. Second, very few studies rigorously test the quality of their uncertainty estimates through statistical adequacy tests and accuracy measures. Third, and most critically for the context of this thesis, the application of probabilistic models to tail risk estimation—particularly Expected Shortfall (ES)—is virtually absent. Of the 62 papers reviewed, only two directly estimate ES, and neither applies formal backtesting procedures to evaluate the adequacy of their forecasts. As such, there remains a substantial gap in the literature regarding the use of probabilistic AI for well-tested, distributional tail risk estimation in financial settings.


%The two dudes forecasting ES:

%risk dude - the methodology focuses entirely on simulation-based validation and efficiency analysis, not on empirical backtesting with actual financial time series or loss data.

%almeida dude - using joint VaR-ES scoring rules, it does not backtest ES forecasts in the traditional sense of regulatory validation

\begin{comment}
  --------------------

Potential benefits with prob AI compared to traditional models include the ability to capture complex non-linear relationships in data and produce full distributional forecasts without explicit parametric assumptions. This is especially important when estimating tail risk, because previous research has shown that financial returns are not normally distributed (Peiro), as is often assumed

There are several ways to make AI models probabilistic, including techniques for quantifying epistemic uncertainty such as variational inference and ensembling, or techniques for quantifying aleatoric uncertainty, such as modifying the models to output random variables rather than point predictions, e.g. through modifying a neural network to output both a mean and a variance and selecting an appropriate loss function [relevant reference] or using mixture density networks [relevant reference]. 

Mixture density networks is a technique that is especially useful for generating non-parametric distributions (with enough mixtures, the model can produce an arbitrarily shaped distribution) ... bla bla (CITE Papers here), maybe talk about some papers using recurrent MDNs (kan si noe om at MDNs som alenestående metode er en gammel ide, og da cite noen gamle papers som snakker om det alene, men at kombinert med en ML approach er det veldig nytt og det som gjør det til Prob AI og at det basically ikke er brukt)... When combining this with a Bayesian inference technique or ensembling of models, it becomes possible to also model the epistemic uncertainty, resulting in a model that can quantify both epistemic and aleatoric uncertainty, without making assumptions about the form of the distributions

Previous attempts ... not good ... especially lacking research on ES

--  
\end{comment}


% Prob AI what is it ... what it has been used for ... something something

% Even though recent surge in machine learning, traditional still dominate in tail risk estimation because it actually works well cite all the stuff from Moen et al., ... however several reports of LSTM and other ML models, bla bla has also proven to work ... cite cite cite ... In our preliminary study we find that probabilistic AI is rapidly evolvling ... due to publication stuff a lot last years ... and that it is promising for tail risk due to modeling capabilities ... but it remains underutilized ... project thesis cite ... maybe add some of the papers there that actually did tail risk estimation and add findings from new stuff perhaps master ... nobody has actually done ES estimation with prob AI.

% Parametric vs non-parametric ... cite assumption review, bayer perhaps ... some more

% Aleatoric epistempc ... nobody does it .. project thesis

% Summary, prob AI has modeling capabiltiies that align with what research has found being important for tail risk estimation, but its novelty makes it hard to say anything about whether it acutally works because noone has done it for ES estimation. 


% A critical methodological choice in probabilistic AI modeling relates to distributional assumptions—parametric versus non-parametric .... more about distributional assumptions - add the review on different distributional assumptions, maybe some more and Bayer,

%  Aleatoric vs epistemic ... nobody does it, refer to project thesis

 

\subsection{Transformers for Financial Time Series Forecasting and Tail Risk}
\label{sec:2.3_transformers}
Transformer architectures, first introduced by \textcite{vaswani2017attention} and initially developed for natural language processing, has proven to be a significant advancement in sequence modeling across various domains. Their potential for financial time series forecasting has recently attracted attention, driven by their capability to capture long-range dependencies and complex temporal dynamics \parencite{lezmi2023time}. Nevertheless, empirical findings regarding their efficacy remain mixed, and only a small body of literature applying them to financial time series forecasting is currently available. \textcite{Sonata2024} suggest that LSTMs networks, due to their inherent memory cell structures, outperform Transformers in modeling long-term financial series, who manage shorter and less structured sequences better. Similarly, \textcite{Zeng2023} identify limitations in the Transformer's self-attention mechanism, emphasizing its potential neglect of essential temporal information, and demonstrate superior performance on several time series using simpler models. In contrast, \textcite{souto2024can} present evidence supporting competitive performance of Transformer-based models in forecasting realized volatility, specifically for stocks within the S\&P 100, directly contesting the findings of \textcite{Zeng2023}.

Concerning tail risk specifically, \textcite{ramos2021multi} demonstrate that hybrid models integrating Transformer and Multi-Transformer architectures significantly enhance accuracy of volatility forecasts for the S\&P 500, and generate VaR estimates at the 99.5\% confidence level under Student-t assumptions that pass Christoffersen's test for adequacy. Utilizing the same models, \textcite{mishra2024volatility} validate their effectiveness for 99\% VaR across multiple asset classes. They also construct ES estimates, but do not assess these statistically, other than concluding that the hybrid models are the most conservative. 

Thus, while some existing studies indicate promising results for Transformer-based approaches, their comparative advantage over established models such as LSTMs remains debated, and further empirical research is necessary to clarify their effectiveness for financial time series forecasting broadly. Moreover, Transformers have not yet been applied comprehensively to ES estimation, nor have they been modeled within a fully probabilistic forecasting framework, as addressed in this thesis.

\subsection{RV and IV for Tail Risk Estimation}
\label{sec:2.4_rv_iv}

Implied Volatility (IV) and Realized Variance (RV) are the two key independent variables employed in this study. Considerable research into their respective roles in volatility forecasting and tail risk estimation have been conducted, and there is an ongoing debate about their relative predictive power.

IV derived from option prices serves as a forward looking measure, reflecting market expectations about future volatility. There is a vast body of literature assessing the predictive power of IV across asset classes. \textcite{Poon2005} provide a comprehensive review of volatility-forecasting methods on a wide range of financial assets, finding that models integrating option-implied volatility often provide more accurate forecasts than time-series models which do not. In the foreign exchange market, where a lot of lot of work regarding implied volatility's predictive power of future realized volatility has been conducted, \textcite{JORION1995} was among the first to show IV's superior predictive ability over statistical models, with later research corroborating these findings \parencite{Charoenwong2009, Busch2011, Plhal2021}. In equity markets, as investigated in this study, \textcite{Christensen1998} demonstrated that S\&P 500 index option-implied volatilities subsume the information in past volatility, yielding more accurate forecasts of subsequent volatility, supported by \textcite{Fleming1998, Blair2001, shu2001relation}. In fact, \textcite{shu2001relation} demonstrate that the S\&P 500’s implied volatility provides significantly better forecasts of future volatility than various historical volatility measures, regardless of the realized volatility estimator used.

While most research has focused on IV's predictive power for realized volatility, its role in tail risk estimation has received less attention. \textcite{Bollerslev2014} linked the variance risk premium to jump tail risk in equity returns, suggesting that the information embedded in the option-implied distribution contain information about tail probabilities. \textcite{giot2005implied} and \textcite{angelidis2008forecasting} found that models incorporating the VIX improved one-day ahead VaR forecasts for equity indices, particularly during periods of market stress, although conflicting evidence exists from \textcite{Chong2004} and \textcite{Kim2015}, who found no significant improvements utilizing implied volatility for VaR estimates in emerging markets. However, research extending these insights to ES remains scarce. A notable exception is \textcite{Lycsa2024} who forecast day-ahead ES on the EUR/USD, noting that evidence for ES predictions using implied volatility prior to their work is virtually non-existent. Their results demonstrate that information provided by forward-looking implied volatility is more valuable than that in backward-looking realized measures. Extending this work, \textcite{moen2024forecasting} confirm that econometric models achieve their best performance when only IV is included as explanatory variable, while machine learning models benefit from combining IV with several volatility measures. Despite these findings, studies directly evaluating IV for ES estimation in equity markets remain limited. 

RV, calculated using high-frequency data, is a backwards-looking measure and has become central to many volatility-forecasting models. The HAR (Heterogeneous Autoregressive) model by \textcite{corsi2009har} leveraging realized volatility data, has been widely adopted due to its proven effectiveness in forecasting future realized volatility \parencite{Bollerslev2016, gong2019modeling}. Although historical RV alone has proven to be a strong predictor, multiple studies suggest that combining RV with IV enhance predictive performance \parencite{Xiao2021,shu2001relation}. In parallel, the integration of RV into more sophisticated econometric frameworks has led to the development of models such as the Realized GARCH by \textcite{Hansen2011}, which jointly models return and realized measures of volatility. This model and further extensions have proven effective in forecasting future volatility \parencite{Gerlach2016, WATANABE2011, Jiang2018}, and has expanded the applicability of RV beyond simple autoregressive structures.

In terms of tail risk, \textcite{Contino2017} demonstrate that Realized GARCH models significantly improve both VaR and ES forecasting for DJIA stocks and ETFs compared to standard GARCH approaches, indicating that RV measures have predictive power in tail risk estimation. Additionally, \textcite{chao2018} show that incorporating realized measures in their joint regression framework of VaR and ES increases performance. However, the aforementioned studies by \textcite{Lycsa2024} and \textcite{moen2024forecasting} suggests that RV may be less informative than IV when predicting ES, with the former highlighting IV’s dominance over RV in forecasting day-ahead ES using econometric models, and the latter showing that combining IV and RV enhances predictive accuracy in more flexible machine learning models . 

In summary, both IV and RV have demonstrated strong predictive power for future volatility, but emerging evidence indicates that IV may hold superior predictive power for ES, particularly in econometric models. However, studies directly evaluating these variables for ES forecasting in equity markets remain sparse, and further research is needed to draw definitive conclusions.

